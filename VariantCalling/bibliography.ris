TY  - JOUR
ID  - Li
DB  - PubMed
AU  - Li, Heng
AU  - Durbin, Richard
T1  - Fast and accurate short read alignment with Burrows-Wheeler transform
LA  - eng
SN  - 1367-4811
SN  - 1367-4803
Y1  - 2009/07/15
ET  - 2009/05/18
AB  - MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. RESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. AVAILABILITY: http://maq.sourceforge.net.
SP  - 1754
EP  - 1760
VL  - 25
IS  - 14
AN  - 19451168
UR  - https://pubmed.ncbi.nlm.nih.gov/19451168
DO  - 10.1093/bioinformatics/btp324
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2705234/
U1  - 19451168[pmid]
U2  - PMC2705234[pmcid]
U4  - btp324[PII]
J2  - Bioinformatics
JF  - Bioinformatics (Oxford, England)
KW  - *Algorithms
KW  - Genomics/*methods
KW  - Sequence Alignment/*methods
KW  - Sequence Analysis, DNA/methods
KW  - *Software
PB  - Oxford University Press
ER  - 
TY  - JOUR
ID  - Mckenna
DB  - PubMed
AU  - McKenna, Aaron
AU  - Hanna, Matthew
AU  - Banks, Eric
AU  - Sivachenko, Andrey
AU  - Cibulskis, Kristian
AU  - Kernytsky, Andrew
AU  - Garimella, Kiran
AU  - Altshuler, David
AU  - Gabriel, Stacey
AU  - Daly, Mark
AU  - DePristo, Mark A
T1  - The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data
LA  - eng
SN  - 1549-5469
SN  - 1088-9051
Y1  - 2010/09/
ET  - 2010/07/19
AB  - Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS--the 1000 Genome pilot alone includes nearly five terabases--make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide polymorphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas.
SP  - 1297
EP  - 1303
VL  - 20
IS  - 9
AN  - 20644199
UR  - https://pubmed.ncbi.nlm.nih.gov/20644199
DO  - 10.1101/gr.107524.110
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2928508/
U1  - 20644199[pmid]
U2  - PMC2928508[pmcid]
U4  - gr.107524.110[PII]
J2  - Genome Res
JF  - Genome research
KW  - Base Sequence
KW  - *Genome
KW  - Genomics/*methods
KW  - Sequence Analysis, DNA/*methods
KW  - *Software
PB  - Cold Spring Harbor Laboratory Press
ER  - 
TY  - JOUR
ID  - Chen
DB  - PubMed
AU  - Chen, Chuming
AU  - Khaleel, Sari S
AU  - Huang, Hongzhan
AU  - Wu, Cathy H
T1  - Software for pre-processing Illumina next-generation sequencing short read sequences
LA  - eng
SN  - 1751-0473
SN  - 1751-0473
Y1  - 2014/05/03
AB  - BACKGROUND: When compared to Sanger sequencing technology, next-generation sequencing (NGS) technologies are hindered by shorter sequence read length, higher base-call error rate, non-uniform coverage, and platform-specific sequencing artifacts. These characteristics lower the quality of their downstream analyses, e.g. de novo and reference-based assembly, by introducing sequencing artifacts and errors that may contribute to incorrect interpretation of data. Although many tools have been developed for quality control and pre-processing of NGS data, none of them provide flexible and comprehensive trimming options in conjunction with parallel processing to expedite pre-processing of large NGS datasets. METHODS: We developed ngsShoRT (next-generation sequencing Short Reads Trimmer), a flexible and comprehensive open-source software package written in Perl that provides a set of algorithms commonly used for pre-processing NGS short read sequences. We compared the features and performance of ngsShoRT with existing tools: CutAdapt, NGS QC Toolkit and Trimmomatic. We also compared the effects of using pre-processed short read sequences generated by different algorithms on de novo and reference-based assembly for three different genomes: Caenorhabditis elegans, Saccharomyces cerevisiae S288c, and Escherichia coli O157 H7. RESULTS: Several combinations of ngsShoRT algorithms were tested on publicly available Illumina GA II, HiSeq 2000, and MiSeq eukaryotic and bacteria genomic short read sequences with the focus on removing sequencing artifacts and low-quality reads and/or bases. Our results show that across three organisms and three sequencing platforms, trimming improved the mean quality scores of trimmed sequences. Using trimmed sequences for de novo and reference-based assembly improved assembly quality as well as assembler performance. In general, ngsShoRT outperformed comparable trimming tools in terms of trimming speed and improvement of de novo and reference-based assembly as measured by assembly contiguity and correctness. CONCLUSIONS: Trimming of short read sequences can improve the quality of de novo and reference-based assembly and assembler performance. The parallel processing capability of ngsShoRT reduces trimming time and improves the memory efficiency when dealing with large datasets. We recommend combining sequencing artifacts removal, and quality score based read filtering and base trimming as the most consistent method for improving sequence quality and downstream assemblies. ngsShoRT source code, user guide and tutorial are available at http://research.bioinformatics.udel.edu/genomics/ngsShoRT/. ngsShoRT can be incorporated as a pre-processing step in genome and transcriptome assembly projects.
SP  - 8
EP  - 8
VL  - 9
AN  - 24955109
UR  - https://pubmed.ncbi.nlm.nih.gov/24955109
DO  - 10.1186/1751-0473-9-8
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4064128/
U1  - 24955109[pmid]
U2  - PMC4064128[pmcid]
U4  - 1751-0473-9-8[PII]
J2  - Source Code Biol Med
JF  - Source code for biology and medicine
KW  - De novo assembly
KW  - Illumina
KW  - Next-generation sequencing
KW  - Perl
KW  - Reference-based assembly
KW  - Trimming
PB  - BioMed Central
ER  - 
TY  - JOUR
ID  - Bolger
DB  - PubMed
AU  - Bolger, Anthony M
AU  - Lohse, Marc
AU  - Usadel, Bjoern
T1  - Trimmomatic: a flexible trimmer for Illumina sequence data
LA  - eng
SN  - 1367-4811
SN  - 1367-4803
Y1  - 2014/08/01
ET  - 2014/04/01
AB  - MOTIVATION: Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. RESULTS: The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested. AVAILABILITY AND IMPLEMENTATION: Trimmomatic is licensed under GPL V3. It is cross-platform (Java 1.5+ required) and available at http://www.usadellab.org/cms/index.php?page=trimmomatic CONTACT: usadel@bio1.rwth-aachen.de SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.
SP  - 2114
EP  - 2120
VL  - 30
IS  - 15
AN  - 24695404
UR  - https://pubmed.ncbi.nlm.nih.gov/24695404
DO  - 10.1093/bioinformatics/btu170
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4103590/
U1  - 24695404[pmid]
U2  - PMC4103590[pmcid]
U4  - btu170[PII]
J2  - Bioinformatics
JF  - Bioinformatics (Oxford, England)
KW  - Computational Biology
KW  - Databases, Genetic
KW  - High-Throughput Nucleotide Sequencing/*methods
KW  - *Software
PB  - Oxford University Press
ER  - 
TY  - JOUR
ID  - Valerie
DB  - PubMed
AU  - Schneider, Valerie A
AU  - Graves-Lindsay, Tina
AU  - Howe, Kerstin
AU  - Bouk, Nathan
AU  - Chen, Hsiu-Chuan
AU  - Kitts, Paul A
AU  - Murphy, Terence D
AU  - Pruitt, Kim D
AU  - Thibaud-Nissen, Françoise
AU  - Albracht, Derek
AU  - Fulton, Robert S
AU  - Kremitzki, Milinn
AU  - Magrini, Vincent
AU  - Markovic, Chris
AU  - McGrath, Sean
AU  - Steinberg, Karyn Meltz
AU  - Auger, Kate
AU  - Chow, William
AU  - Collins, Joanna
AU  - Harden, Glenn
AU  - Hubbard, Timothy
AU  - Pelan, Sarah
AU  - Simpson, Jared T
AU  - Threadgold, Glen
AU  - Torrance, James
AU  - Wood, Jonathan M
AU  - Clarke, Laura
AU  - Koren, Sergey
AU  - Boitano, Matthew
AU  - Peluso, Paul
AU  - Li, Heng
AU  - Chin, Chen-Shan
AU  - Phillippy, Adam M
AU  - Durbin, Richard
AU  - Wilson, Richard K
AU  - Flicek, Paul
AU  - Eichler, Evan E
AU  - Church, Deanna M
T1  - Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly
LA  - eng
SN  - 1549-5469
SN  - 1088-9051
Y1  - 2017/05/
ET  - 2017/04/10
AB  - The human reference genome assembly plays a central role in nearly all aspects of today's basic and clinical research. GRCh38 is the first coordinate-changing assembly update since 2009; it reflects the resolution of roughly 1000 issues and encompasses modifications ranging from thousands of single base changes to megabase-scale path reorganizations, gap closures, and localization of previously orphaned sequences. We developed a new approach to sequence generation for targeted base updates and used data from new genome mapping technologies and single haplotype resources to identify and resolve larger assembly issues. For the first time, the reference assembly contains sequence-based representations for the centromeres. We also expanded the number of alternate loci to create a reference that provides a more robust representation of human population variation. We demonstrate that the updates render the reference an improved annotation substrate, alter read alignments in unchanged regions, and impact variant interpretation at clinically relevant loci. We additionally evaluated a collection of new de novo long-read haploid assemblies and conclude that although the new assemblies compare favorably to the reference with respect to continuity, error rate, and gene completeness, the reference still provides the best representation for complex genomic regions and coding sequences. We assert that the collected updates in GRCh38 make the newer assembly a more robust substrate for comprehensive analyses that will promote our understanding of human biology and advance our efforts to improve health.
SP  - 849
EP  - 864
VL  - 27
IS  - 5
AN  - 28396521
UR  - https://pubmed.ncbi.nlm.nih.gov/28396521
DO  - 10.1101/gr.213611.116
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5411779/
U1  - 28396521[pmid]
U2  - PMC5411779[pmcid]
U4  - gr.213611.116[PII]
J2  - Genome Res
JF  - Genome research
KW  - Contig Mapping/*methods/standards
KW  - *Genome, Human
KW  - Genomics/*methods/standards
KW  - Haploidy
KW  - Haplotypes
KW  - Humans
KW  - Polymorphism, Genetic
KW  - Reference Standards
KW  - Sequence Analysis, DNA/*methods/standards
KW  - *Software
PB  - Cold Spring Harbor Laboratory Press
ER  - 
TY  - JOUR
ID  - Kalinin
DB  - PubMed
AU  - Kalinin, Alexandr A
AU  - Higgins, Gerald A
AU  - Reamaroon, Narathip
AU  - Soroushmehr, Sayedmohammadreza
AU  - Allyn-Feuer, Ari
AU  - Dinov, Ivo D
AU  - Najarian, Kayvan
AU  - Athey, Brian D
T1  - Deep learning in pharmacogenomics: from gene regulation to patient stratification
LA  - eng
SN  - 1744-8042
SN  - 1462-2416
Y1  - 2018/05/
ET  - 2018/04/26
AB  - This Perspective provides examples of current and future applications of deep learning in pharmacogenomics, including: identification of novel regulatory variants located in noncoding domains of the genome and their function as applied to pharmacoepigenomics; patient stratification from medical records; and the mechanistic prediction of drug response, targets and their interactions. Deep learning encapsulates a family of machine learning algorithms that has transformed many important subfields of artificial intelligence over the last decade, and has demonstrated breakthrough performance improvements on a wide range of tasks in biomedicine. We anticipate that in the future, deep learning will be widely used to predict personalized drug response and optimize medication selection and dosing, using knowledge extracted from large and complex molecular, epidemiological, clinical and demographic datasets.
SP  - 629
EP  - 650
VL  - 19
IS  - 7
AN  - 29697304
UR  - https://pubmed.ncbi.nlm.nih.gov/29697304
DO  - 10.2217/pgs-2018-0008
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6022084/
U1  - 29697304[pmid]
U2  - PMC6022084[pmcid]
J2  - Pharmacogenomics
JF  - Pharmacogenomics
KW  - *adverse events
KW  - *artificial intelligence
KW  - *deep learning
KW  - *drug discovery
KW  - *drug–drug interaction
KW  - *drug–gene interaction
KW  - *noncoding regulatory variation
KW  - *patient stratification
KW  - *pharmacogenomics
KW  - Algorithms
KW  - Databases as Topic
KW  - *Deep Learning/trends
KW  - Humans
KW  - *Models, Educational
KW  - Neural Networks, Computer
KW  - Pharmacogenetics/*education/*trends
PB  - Future Medicine Ltd
ER  - 
TY  - JOUR
ID  - Lisa
DB  - PubMed
AU  - Johnson, Lisa K
AU  - Alexander, Harriet
AU  - Brown, C Titus
T1  - Re-assembly, quality evaluation, and annotation of 678 microbial eukaryotic reference transcriptomes
LA  - eng
SN  - 2047-217X
Y1  - 2019/04/01
AB  - BACKGROUND: De novo transcriptome assemblies are required prior to analyzing RNA sequencing data from a species without an existing reference genome or transcriptome. Despite the prevalence of transcriptomic studies, the effects of using different workflows, or "pipelines," on the resulting assemblies are poorly understood. Here, a pipeline was programmatically automated and used to assemble and annotate raw transcriptomic short-read data collected as part of the Marine Microbial Eukaryotic Transcriptome Sequencing Project. The resulting transcriptome assemblies were evaluated and compared against assemblies that were previously generated with a different pipeline developed by the National Center for Genome Research. RESULTS: New transcriptome assemblies contained the majority of previous contigs as well as new content. On average, 7.8% of the annotated contigs in the new assemblies were novel gene names not found in the previous assemblies. Taxonomic trends were observed in the assembly metrics. Assemblies from the Dinoflagellata showed a higher number of contigs and unique k-mers than transcriptomes from other phyla, while assemblies from Ciliophora had a lower percentage of open reading frames compared to other phyla. CONCLUSIONS: Given current bioinformatics approaches, there is no single "best" reference transcriptome for a particular set of raw data. As the optimum transcriptome is a moving target, improving (or not) with new tools and approaches, automated and programmable pipelines are invaluable for managing the computationally intensive tasks required for re-processing large sets of samples with revised pipelines and ensuring a common evaluation workflow is applied to all samples. Thus, re-assembling existing data with new tools using automated and programmable pipelines may yield more accurate identification of taxon-specific trends across samples in addition to novel and useful products for the community.
SP  - giy158
VL  - 8
IS  - 4
AN  - 30544207
UR  - https://pubmed.ncbi.nlm.nih.gov/30544207
DO  - 10.1093/gigascience/giy158
L2  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6481552/
U1  - 30544207[pmid]
U2  - PMC6481552[pmcid]
U4  - 5241890[PII]
J2  - Gigascience
JF  - GigaScience
KW  - *automated pipeline
KW  - *marine microbial eukaryote
KW  - *re-analysis
KW  - *transcriptome assembly
KW  - *Computational Biology/methods
KW  - Databases, Genetic
KW  - Eukaryota/*genetics
KW  - *Gene Expression Profiling/methods
KW  - Genome
KW  - Genomics/methods
KW  - High-Throughput Nucleotide Sequencing
KW  - *Transcriptome
KW  - Workflow
PB  - Oxford University Press
ER  - 
